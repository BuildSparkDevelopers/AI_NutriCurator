# -*- coding: utf-8 -*-
"""LangGraphArchitecture.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h71SizaqMrE640Jpv73CgDps5MysOo2o

#âš™ï¸â€‹ ëª¨ë¸ ë¡œë“œ

tool callingì„ ìœ„í•œ ë˜í•‘ í•„ìš” - ì±—ì§€í”¼í‹° ëŒ€í™” ì°¸ê³ 
"""

#ëª¨ë¸ ë¡œë“œ ì´ˆê¸°í™”
import torch
import gc

# (Jupyter Notebook í™˜ê²½ì—ì„œ ëª¨ë¸ ê°ì²´ ì‚­ì œí•˜ë˜ ë¶€ë¶„ ìŠ¤í¬ë¦½íŠ¸í™” ì‹œ ì—ëŸ¬ë¡œ ì£¼ì„ì²˜ë¦¬)
# del model
# ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
gc.collect()

# GPU ìºì‹œ ë¹„ìš°ê¸° (PyTorch ì‚¬ìš© ì‹œ)
torch.cuda.empty_cache()

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

#ëª¨ë¸ë¡œë“œ ê¸°ë³¸ì˜µì…˜ - íì›¬ìš©

MODEL_NAME = "Qwen/Qwen2.5-14B-Instruct"
#MODEL_NAME = "Qwen/Qwen2.5-32B-Instruct"
#MODEL_NAME = "Qwen/Qwen2.5-72B-Instruct"
#MODEL_NAME = "google/gemma-3-27b-it"
#MODEL_NAME = "openai/gpt-oss-20b"

tokenizer = AutoTokenizer.from_pretrained(
    MODEL_NAME,
    trust_remote_code=True
)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.bfloat16,  #í”Œë¡¯ ì•ì— bì¶”ê°€
    device_map="auto",
    trust_remote_code=True
)

model.eval()


"""# ğŸ§¾ ë°ì´í„° ìŠ¤í‚¤ë§ˆ"""

from typing import List, TypedDict, Literal
from decimal import Decimal
from enum import Enum

# 1. ì¤‘ë³µì ìš©ì´ ê°€ëŠ¥í•œ ì§ˆë³‘ ì •ë³´
class DiseaseInfo(TypedDict):
    group: str           # 4ê°œ êµ° ì´ë¦„ (ì˜ˆ: 'Allergy', 'Diabetes', 'Hypertension', 'Kidney disorder' ë“±)
    step: int            # ì„¸ë¶€ ë‹¨ê³„ (ì˜ˆ: 1, 2, 3)
    description: str     # í•´ë‹¹ ë‹¨ê³„ì— ëŒ€í•œ ê°„ëµí•œ ì„¤ëª… (ì„ íƒ ì‚¬í•­)

# 2. ë©”ì¸ í´ë˜ìŠ¤ (State)
class overallState(TypedDict):
    user_id: str
    product_id: str
    name: str

    # [User-Agentê°€ ì±„ì›Œì¤„ ì •ë³´]
    diabetes_flag: int
    hypertension_flag: int
    kidneydisease_flag: int
    allergy_flag: int

    diabetes_detail: Literal["DIABETES"] | None #"TYPE_1", "TYPE_2", "GESTATIONAL"
    hypertension_detail: Literal["HYPERTENSION"] | None #"STAGE_1", "STAGE_2"
    kidney_detail: Literal["CKD_3_5", "HD", "PD"] | None
    allergy_list: List[str]

    #guidelines: List[str]
    final_profile: dict

    # [chat-Agentê°€ ì±„ì›Œì¤„ ì •ë³´]
    any_exceed : bool
    exceeded_nutrients: List[str]

    any_allergen: bool          # ì œí’ˆì— ì•ŒëŸ¬ì§€ ìœ ë°œ ë¬¼ì§ˆì´ í¬í•¨ë˜ì—ˆëŠ”ê°€?
    substitute: List[str]      # ì¶”ì²œ ëŒ€ì²´ ì‹ì¬ë£Œ ëª©ë¡


    allergy_safety_summary: str
    warniing : bool

    # [íë¦„ ì œì–´]
    next_step: str
    final_answer: str              # ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ìµœì¢… ë‹µë³€



class KFDAAllergen(str, Enum):
    """ì‹ì•½ì²˜ ê³ ì‹œ ì•Œë ˆë¥´ê¸° ìœ ë°œë¬¼ì§ˆ 22ì¢…"""
    EGG = "ë‚œë¥˜(ê°€ê¸ˆë¥˜)"
    MILK = "ìš°ìœ "
    BUCKWHEAT = "ë©”ë°€"
    PEANUT = "ë•…ì½©"
    SOYBEAN = "ëŒ€ë‘"
    WHEAT = "ë°€"
    MACKEREL = "ê³ ë“±ì–´"
    CRAB = "ê²Œ"
    SHRIMP = "ìƒˆìš°"
    PORK = "ë¼ì§€ê³ ê¸°"
    PEACH = "ë³µìˆ­ì•„"
    TOMATO = "í† ë§ˆí† "
    SULFITE = "ì•„í™©ì‚°ë¥˜"
    WALNUT = "í˜¸ë‘"
    CHICKEN = "ë‹­ê³ ê¸°"
    BEEF = "ì‡ ê³ ê¸°"
    SQUID = "ì˜¤ì§•ì–´"
    SHELLFISH = "ì¡°ê°œë¥˜(êµ´, ì „ë³µ, í™í•© í¬í•¨)"
    PINE_NUT = "ì£"
    # ... (ë‚˜ë¨¸ì§€ í¬í•¨ ì´ 22ì¢…)

# íŒŒì´ë„ í”„ë¡œí•„ ìƒì„± ë©”ì„œë“œ
# LangGraph ì‹¤í–‰ë‹¨ì—ì„œëŠ” DBì— ì €ì¥ë˜ì–´ ìˆìŒì„ ê°€ì •í•¨
# final_profile: dict

def generate_final_profile(user_id, user_diseases, user_weight):
    # 1. ì§ˆí™˜ë³„ ì„ê³„ê°’ ì„¤ì • (Data Dictionary)
    # ìˆ˜ì¹˜ ë’¤ì˜ 'g', 'mg' ë“±ì˜ ë‹¨ìœ„ëŠ” ê³„ì‚° í¸ì˜ë¥¼ ìœ„í•´ ìƒëµí•©ë‹ˆë‹¤.
    disease_thresholds = {
        "allergy": {
            "restricted_ingredients": ["milk", "egg", "peanut", "nuts", "soy", "wheat", "fish", "shellfish"]
        },
        "kidneydisease_pre_dialysis": {  # CKD 3-5ë‹¨ê³„ (íˆ¬ì„ ì „)
            "protein": 0.20 * user_weight, # kgë‹¹ ê³„ì‚° (1ì¼ ê¸°ì¤€: 0.60 * user_weight)
            "sodium": 766.67,              # 1ì¼ ê¸°ì¤€: 2300
            "phosphorus": 333.33,          # 1ì¼ ê¸°ì¤€: 1000
            "calcium": 333.33              # 1ì¼ ê¸°ì¤€: 1000
        },
        "kidneydisease_dialysis": {      # CKD 5ë‹¨ê³„ (íˆ¬ì„)
            "protein": 0.40 * user_weight, # kgë‹¹ ê³„ì‚° (1ì¼ ê¸°ì¤€: 1.2 * user_weight)
            "sodium": 766.67,              # 1ì¼ ê¸°ì¤€: 2300
            "potassium": 666.67,           # 1ì¼ ê¸°ì¤€: 2000
            "phosphorus": 333.33,          # 1ì¼ ê¸°ì¤€: 1000
            "calcium": 333.33              # 1ì¼ ê¸°ì¤€: 1000
        },
        "diabetes": {
            "sugar": 1.67                  # 1ì¼ ê¸°ì¤€: 5
        },
        "hypertension": {
            "sodium": 766.67,              # 1ì¼ ê¸°ì¤€: 2300
            "potassium_min": 1166.67,      # > 1166.67mg (1ì¼ ê¸°ì¤€: 3500)
            "fat_ratio": 0.25              # ì´ ì—´ëŸ‰ì˜ 25% ì´í•˜ (ë¹„ìœ¨ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ìœ ì§€)
        }
    }

    # 2. ìš°ì„ ìˆœìœ„ ë§µ (ë‚®ì„ìˆ˜ë¡ ë†’ìŒ)
    priority_map = {
        "allergy": 1,
        "kidneydisease": 2,
        "diabetes": 3,
        "hypertension": 4
    }

    # 3. ì‚¬ìš©ìê°€ ê°€ì§„ ì§ˆí™˜ í•„í„°ë§ ë° ìš°ì„ ìˆœìœ„ ì •ë ¬
    # ì˜ˆ: user_diseases = {"diabetes": 1, "kidneydisease": 1}
    active_diseases = [d for d, active in user_diseases.items() if active == 1]
    sorted_diseases = sorted(active_diseases, key=lambda x: priority_map.get(x, 99))

    # 4. Final Profile ìƒì„± (Priority-Merger)
    final_profile = {
        "user_id": user_id,
        "restricted_ingredients": []
    }

    for disease in sorted_diseases:
        # ì‹ ì¥ë³‘ì˜ ê²½ìš° ì„¸ë¶€ ë‹¨ê³„(íˆ¬ì„ ì—¬ë¶€)ì— ë”°ë¥¸ ë¶„ê¸° ì²˜ë¦¬ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œë¡œ pre_dialysisë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        lookup_key = "kidneydisease_pre_dialysis" if disease == "kidneydisease" else disease
        thresholds = disease_thresholds.get(lookup_key, {})

        for nutrient, value in thresholds.items():
            # ì•ŒëŸ¬ì§€ ìœ ë°œ ë¬¼ì§ˆ ë¬¸ìì—´ ì²˜ë¦¬
            if nutrient == "restricted_ingredients":
                # ì¤‘ë³µ ì—†ì´ ì¶”ê°€
                final_profile["restricted_ingredients"] = list(set(final_profile["restricted_ingredients"] + value))

            # ì„±ë¶„ ì„ê³„ê°’ ì²˜ë¦¬: ì´ë¯¸ ë“±ë¡ëœ ì„±ë¶„ì€ ë¬´ì‹œ (ìš°ì„ ìˆœìœ„ ë³´í˜¸)
            elif nutrient not in final_profile:
                final_profile[nutrient] = value

    return final_profile

# --- ì‹¤í–‰ ì˜ˆì‹œ ---
user_id_a = "patient_A"
user_weight_a = 70 # kg
user_diseases_a = {
    "diabetes": 1,
    "hypertension": 0,
    "kidneydisease": 1,
    "allergy": 0
}

profile = generate_final_profile(user_id_a, user_diseases_a, user_weight_a)

import json
print(json.dumps(profile, indent=4, ensure_ascii=False))

# ì‹í’ˆ ì„±ë¶„ ë§¤í•‘ í…Œì´ë¸” ì •ì˜

# final_profile í‚¤ : ì‹í’ˆ DB ì»¬ëŸ¼ëª… ë§¤í•‘
FOOD_DB_MAPPER = {
    "calories" : "ì—ë„ˆì§€(kcal)",
    "sodium" : "ë‚˜íŠ¸ë¥¨(mg)",
    "carbohydrate" : "íƒ„ìˆ˜í™”ë¬¼(g)",
    "sugar" : "ë‹¹ë¥˜(g)",
    "fat": "ì§€ë°©(g)",
    "trans fat" : "íŠ¸ëœìŠ¤ì§€ë°©(g)",
    "saturated fat" : "í¬í™”ì§€ë°©(g)",
    "cholesterol" : "ì½œë ˆìŠ¤í…Œë¡¤(mg)",
    "protein" : "ë‹¨ë°±ì§ˆ(g)",
    "phosphorus" : "ì¸(mg)",
    "calcium" : "ì¹¼ìŠ˜(mg)",
    "potassium" : "ì¹¼ë¥¨(mg)"
    # ì‹í’ˆí‘œì‹œ ì˜ë¬´ëŒ€ìƒ ë° disease_thresholdsì—ì„œ ì‚¬ìš©ëœ ì„±ë¶„ì„ ì „ë¶€ ë§¤í•‘í•¨
    # ì™¼ìª½ ì¹¼ëŸ¼ ì¶œì²˜: disease_thresholds
    # ì˜¤ë¥¸ìª½ ì¹¼ëŸ¼ ì¶œ: ì‹í’ˆì˜ì•½í’ˆ ì•ˆì „ì²˜ì—ì„œ ì œê³µí•˜ëŠ” "20251229_ìŒì‹DB 19495ê±´"
}

"""#ğŸ§¾ JSON ë°ì´í„°"""

products = {
    "0": {
        "product_id": "201905000000",
        "name": "ì„¤í™”ëˆˆê½ƒíŒê¹€ë¶€ê°ìŠ¤ë‚µ",
        "category": "ê³¼ì",
        "ingredients": ["ì°¹ìŒ€", "ê¹€", "ì°¸ê¹¨", "ì˜¥ìˆ˜ìˆ˜ê¸°ë¦„", "ì–‘íŒŒ", "ë¬´", "ëŒ€íŒŒ", "ì²œì¼ì—¼", "ë§ˆëŠ˜", "ìƒˆìš°", "ë©¸ì¹˜", "ë‹¤ì‹œë§ˆ", "ê±´í‘œê³ ë²„ì„¯", "ë‘¥êµ´ë ˆ", "ê°ì´ˆ", "ì •ì œìˆ˜"],
        "allergy": "ì—†ìŒ",
        "trace": "null",
        "calories": 150, "sodium": 180, "carbohydrate": 20, "sugar": 1,
        "fat": 7, "trans_fat": 0, "saturated_fat": 1.2, "cholesterol": 5,
        "protein": 2, "phosphorus": 45, "calcium": 30, "potassium": 120
    },
    "1": {
        "product_id": "201804000001",
        "name": "ì„¤í™”ëˆˆê½ƒíŒê¹€ë¶€ê°ìŠ¤ë‚µ ì•„ëª¬ë“œë§›",
        "category": "ê³¼ì",
        "ingredients": ["ì°¹ìŒ€", "ê¹€", "ì°¸ê¹¨", "ì•„ëª¬ë“œ", "ì–‘íŒŒ", "ë¬´", "ì²œì¼ì—¼", "ìƒˆìš°", "ë©¸ì¹˜", "ë‹¤ì‹œë§ˆ", "ì •ì œìˆ˜"],
        "allergy": "ì•„ëª¬ë“œ",
        "trace": "",
        "calories": 170, "sodium": 160, "carbohydrate": 18, "sugar": 1,
        "fat": 10, "trans_fat": 0, "saturated_fat": 1.0, "cholesterol": 3,
        "protein": 4, "phosphorus": 80, "calcium": 50, "potassium": 200
    },
    "2": {
        "product_id": "201804000002",
        "name": "ê³ ë“¤ë¹¼ê¸°ê¹€ì¹˜",
        "category": "ê¹€ì¹˜ë¥˜",
        "ingredients": ["ê³ ë“¤ë¹¼ê¸°", "ë©¸ì¹˜ì•¡", "ì—¼ì¥ìƒˆìš°", "ì–‘íŒŒ", "í˜¼í•©ê°„ì¥", "ê³ ì¶§ê°€ë£¨", "ë§ˆëŠ˜", "ì°¸ê¹¨", "ë¬¼ì—¿", "ë°°ì¦™", "ë‹¹ê·¼"],
        "allergy": "ìƒˆìš°,ëŒ€ë‘,ë°€",
        "trace": "ë°€, ë•…ì½©, ë³µìˆ­ì•„, í† ë§ˆí† , í˜¸ë‘, ì•„í™©ì‚°ë¥˜ í˜¼ì… ê°€ëŠ¥",
        "calories": 45, "sodium": 850, "carbohydrate": 8, "sugar": 4,
        "fat": 0.5, "trans_fat": 0, "saturated_fat": 0.1, "cholesterol": 2,
        "protein": 2, "phosphorus": 35, "calcium": 40, "potassium": 320
    },
    "3": {
        "product_id": "199504000000",
        "name": "í•´íƒœ í—ˆë‹ˆë²„í„°ì¹©",
        "category": "ê³¼ì",
        "ingredients": ["ê°ì", "í˜¼í•©ì‹ìš©ìœ ", "í—ˆë‹ˆë²„í„°ë§›ì‹œì¦ˆë‹", "íƒˆì§€ë¶„ìœ (ìš°ìœ )", "ë²„í„°í˜¼í•©ë¶„ë§(ëŒ€ë‘)", "ì•„ì¹´ì‹œì•„ê¿€", "ê³ ë©”ë²„í„°(ë°€)"],
        "allergy": "ì•Œìˆ˜ì—†ìŒ",
        "trace": "null",
        "calories": 345, "sodium": 350, "carbohydrate": 30, "sugar": 7,
        "fat": 23, "trans_fat": 0.1, "saturated_fat": 8, "cholesterol": 10,
        "protein": 3, "phosphorus": 70, "calcium": 25, "potassium": 450
    },
    "4": {
        "product_id": "201405000000",
        "name": "í—¬ë¡œë²„ë¸” ë¼ì´ìŠ¤í¼í”„ ì–‘íŒŒë§›",
        "ingredients": ["í˜„ë¯¸", "ì–´ë‹ˆì–¸ì‹œì¦ˆë‹", "ì •ë°±ë‹¹", "ë¦¬ì¹˜ë²„í„°ë¶„ë§", "ì–‘íŒŒë¶„", "í•©ì„±í–¥ë£Œ", "ë°±ë¯¸", "ì˜¥ìˆ˜ìˆ˜ê³¼ë¦½"],
        "allergy": "ì•Œìˆ˜ì—†ìŒ",
        "trace": "ë¼ì§€ê³ ê¸°, ë•…ì½©, ë³µìˆ­ì•„, ì•„í™©ì‚°ë¥˜, í˜¸ë‘ í˜¼ì… ê°€ëŠ¥",
        "calories": 120, "sodium": 110, "carbohydrate": 22, "sugar": 3,
        "fat": 2.5, "trans_fat": 0, "saturated_fat": 0.5, "cholesterol": 0,
        "protein": 2, "phosphorus": 55, "calcium": 15, "potassium": 90
    },
    "5": {
        "product_id": "201105000000",
        "name": "ë‘ë§ˆë¦¬ëª©ì¥ ì½œë¹„ì¹˜ì¦ˆ",
        "ingredients": ["ì›ìœ (êµ­ì‚°)99.9%", "ìš°ìœ ì‘ê³ íš¨ì†Œ", "ìœ ì‚°ê· ", "ì‹ì—¼", "ì•ˆë‚˜í† ìƒ‰ì†Œ"],
        "allergy": "ìš°ìœ í•¨ìœ ",
        "trace": "null",
        "calories": 115, "sodium": 190, "carbohydrate": 1, "sugar": 0.5,
        "fat": 9, "trans_fat": 0.3, "saturated_fat": 6, "cholesterol": 30,
        "protein": 7, "phosphorus": 140, "calcium": 210, "potassium": 25
    },
    "6": {
        "product_id": "201105000001",
        "name": "ì–‘ë°˜ ë°”ì‚­ íŠ€ê¹€ê°€ë£¨",
        "ingredients": ["ë°€ê°€ë£¨", "ë³€ì„±ì „ë¶„", "ë² ì´í‚¹íŒŒìš°ë”", "ì •ì œì†Œê¸ˆ", "ì–‘íŒŒë¶„ë§", "ì˜¥ìˆ˜ìˆ˜ê°€ë£¨"],
        "allergy": "ë°€",
        "trace": "null",
        "calories": 350, "sodium": 650, "carbohydrate": 78, "sugar": 2,
        "fat": 1.2, "trans_fat": 0, "saturated_fat": 0.3, "cholesterol": 0,
        "protein": 7, "phosphorus": 95, "calcium": 20, "potassium": 110
    },
    "7": {
        "product_id": "201104000001",
        "name": "ëˆëª©ì‚´í›ˆì œë°”ë² íìŠ¤í…Œì´í¬",
        "ingredients": ["ë¼ì§€ê³ ê¸° 96.68%", "ìŠ¤ëª¨í¬ì‹œì¦ˆë‹", "ë¶„ë¦¬ëŒ€ë‘ë‹¨ë°±", "í† ë§ˆí† ì¼€ì°¹", "ì•„ì§ˆì‚°ë‚˜íŠ¸ë¥¨"],
        "allergy": "ë¼ì§€ê³ ê¸°,ë°€,ìš°ìœ ,ëŒ€ë‘,ì‡ ê³ ê¸°,í† ë§ˆí†  í•¨ìœ ",
        "trace": "null",
        "calories": 280, "sodium": 720, "carbohydrate": 5, "sugar": 2,
        "fat": 20, "trans_fat": 0, "saturated_fat": 7, "cholesterol": 65,
        "protein": 19, "phosphorus": 180, "calcium": 15, "potassium": 310
    },
    "8": {
        "product_id": "201104000002",
        "name": "íƒœì–‘ì´ˆ ê³ ì¶”ì¥ ê³¨ë“œ",
        "ingredients": ["ê³ ì¶§ê°€ë£¨", "ë¬¼ì—¿", "ì†Œë§¥ë¶„(ë°€)", "í˜¼í•©ì–‘ë…", "ë°€ìŒ€", "ì •ì œì†Œê¸ˆ", "ì •ë°±ë‹¹"],
        "allergy": "ì•Œìˆ˜ì—†ìŒ",
        "trace": "null",
        "calories": 210, "sodium": 2400, "carbohydrate": 48, "sugar": 25,
        "fat": 1, "trans_fat": 0, "saturated_fat": 0.2, "cholesterol": 0,
        "protein": 4, "phosphorus": 90, "calcium": 35, "potassium": 450
    },
    "9": {
        "product_id": "201104000003",
        "name": "í™˜íƒ€ì§€ ë¯¹ìŠ¤ë„ˆíŠ¸",
        "ingredients": ["ì»¤í”¼ë•…ì½©", "í™”ì´íŠ¸ë³¼", "ì°¹ìŒ€ë•…ì½©", "íŠ€ê¹€ë•…ì½©", "ë¡œìŠ¤í‹°ë“œí”¼ë„ˆì¸ ", "ë°”ë‚˜ë‚˜ì¹©", "ë³¶ìŒì•„ëª¬ë“œ", "ê¿€ë•…ì½©"],
        "allergy": "ì•Œìˆ˜ì—†ìŒ",
        "trace": "null",
        "calories": 520, "sodium": 280, "carbohydrate": 45, "sugar": 18,
        "fat": 34, "trans_fat": 0, "saturated_fat": 9, "cholesterol": 0,
        "protein": 14, "phosphorus": 310, "calcium": 75, "potassium": 580
    }
}





# í…ŒìŠ¤íŠ¸ìš© ìœ ì € íŒŒì´ë„í”„ë¡œí•„ (ë‹¹ë‡¨ + ì‹ ì¥ë³‘ í™˜ì ê°€ì •)
final_profiles = {
    "0": { # ë‹¹ë‡¨ + ìš°ìœ /ë•…ì½© ì•ŒëŸ¬ì§€
        "restricted_ingredients": ["ìš°ìœ ", "ë•…ì½©"],
        "sugar": 5.0
    },
    "1": { # ê³ í˜ˆì•• + ìƒˆìš° ì•ŒëŸ¬ì§€
        "restricted_ingredients": ["ìƒˆìš°"],
        "sodium": 2300.0,
        "potassium": 3500.0,
        "fat_ratio": 0.25
    },
    "2": { # íˆ¬ì„ ì „ ì‹ ì¥ì§ˆí™˜ (70kg ê¸°ì¤€)
        "restricted_ingredients": [],
        "protein": 42.0,  # 0.6 * 70
        "sodium": 2300.0,
        "phosphorus": 1000.0,
        "calcium": 1000.0
    },
    "3": { # íˆ¬ì„ ì¤‘ ì‹ ì¥ì§ˆí™˜ + ë°€ ì•ŒëŸ¬ì§€
        "restricted_ingredients": ["ë°€"],
        "protein": 84.0,  # 1.2 * 70
        "sodium": 2300.0,
        "potassium": 2000.0,
        "phosphorus": 1000.0
    },
    "4": { # ë‹¹ë‡¨ + ê³ í˜ˆì•• ë³µí•© (ê°€ì¥ í”í•œ ì¼€ì´ìŠ¤)
        "restricted_ingredients": [],
        "sugar": 5.0,
        "sodium": 2300.0,
        "potassium": 3500.0,
        "fat_ratio": 0.25
    },
    "5": { # ë³µí•© ì•ŒëŸ¬ì§€ (ìœ ì œí’ˆ, ê³„ë€, ê²¬ê³¼ë¥˜)
        "restricted_ingredients": ["ìš°ìœ ", "ê³„ë€", "ê²¬ê³¼ë¥˜"],

    },
    "6": { # ë‹¹ë‡¨ + íˆ¬ì„ ì „ ì‹ ì¥ì§ˆí™˜ + ëŒ€ë‘ ì•ŒëŸ¬ì§€
        "restricted_ingredients": ["ëŒ€ë‘"],
        "sugar": 5.0,
        "protein": 42.0,
        "sodium": 2300.0,
        "phosphorus": 1000.0
    },
    "7": { # ê³ í˜ˆì•• + ìƒì„ /ì¡°ê°œë¥˜ ì•ŒëŸ¬ì§€
        "restricted_ingredients": ["ê³ ë“±ì–´", "ì¡°ê°œ"],
        "sodium": 2300.0,
        "potassium": 3500.0,
        "fat_ratio": 0.25
    },
    "user_8": { # ëª¨ë“  ì§ˆí™˜ ë³µí•© (ìµœì•…ì˜ ì‹œë‚˜ë¦¬ì˜¤ - ê³ ìœ„í—˜êµ°)
        "restricted_ingredients": ["ë•…ì½©", "ë°€"],
        "sugar": 5.0,
        "protein": 42.0,
        "sodium": 2300.0,
        "potassium": 2000.0, # ì‹ ì¥ì§ˆí™˜ ê¸°ì¤€ ì ìš©
        "phosphorus": 1000.0,
        "fat_ratio": 0.25
    },
    "9": { # ê³ ë ¹ì íƒ€ê²Ÿ (ê³ í˜ˆì•• + íˆ¬ì„ ì¤‘ ì‹ ì¥ì§ˆí™˜)
        "restricted_ingredients": [],
        "protein": 84.0,
        "sodium": 2300.0,
        "potassium": 2000.0,
        "phosphorus": 1000.0,
        "fat_ratio": 0.25
    }
}

allergy_substitution_rules = {
    "description": "ì‹ì•½ì²˜ ê³ ì‹œ 22ì¢… ì•Œë ˆë¥´ê¸° ìœ ë°œ ë¬¼ì§ˆë³„ ëŒ€ì²´ ì‹ì¬ë£Œ ì¶”ì²œ ê°€ì´ë“œ",
    "rules": [
      {
        "category": "ë‚œë¥˜",
        "items": ["ê³„ë€", "ë©”ì¶”ë¦¬ì•Œ", "ì˜¤ë¦¬ì•Œ"],
        "substitutes": ["ë‘ë¶€", "ë³‘ì•„ë¦¬ì½© ê±°í’ˆ(ì•„ì¿ ì•„íŒŒë°”)", "ì¹˜ì•„ì”¨ë“œ í˜ì´ìŠ¤íŠ¸", "ê°•ë‚­ì½©"],
        "usage_tip": "ë² ì´í‚¹ ì‹œ ê³„ë€ì˜ ê²°í•©ë ¥ì€ ì•„ì¿ ì•„íŒŒë°”ë‚˜ ë°”ë‚˜ë‚˜ë¡œ ëŒ€ì²´ ê°€ëŠ¥í•©ë‹ˆë‹¤."
      },
      {
        "category": "ìš°ìœ ",
        "items": ["ìš°ìœ ", "ì‚°ì–‘ìœ ", "ì›ìœ ", "íƒˆì§€ë¶„ìœ ", "í™˜ì›ìœ ", "í™˜ì›ë¬´ì§€ë°©ìš°ìœ ", "ê°€ê³µìœ "],
        "substitutes": ["ë‘ìœ ", "ì•„ëª¬ë“œìœ ", "ì˜¤íŠ¸ìœ ", "ìŒ€ìŒë£Œ", "ì½”ì½”ë„›ë°€í¬"],
        "usage_tip": "í¬ë¦¬ë¯¸í•œ ì§ˆê°ì„ ì›í•  ê²½ìš° ì½”ì½”ë„›ë°€í¬ë‚˜ ìºìŠˆë„› ë°€í¬ê°€ ì í•©í•©ë‹ˆë‹¤."
      },
      {
        "category": "ë©”ë°€",
        "items": ["ë©”ë°€"],
        "substitutes": ["ìŒ€", "ë°€", "ê°ì ì „ë¶„", "ê³ êµ¬ë§ˆ ì „ë¶„"],
        "usage_tip": "ë©´ ìš”ë¦¬ ì‹œ ìŒ€ë©´ì´ë‚˜ ì „ë¶„ í•¨ëŸ‰ì´ ë†’ì€ ë©´ìœ¼ë¡œ ì«„ê¹ƒí•¨ì„ ëŒ€ì²´í•©ë‹ˆë‹¤."
      },
      {
        "category": "ë•…ì½©",
        "items": ["ë•…ì½©"],
        "substitutes": ["í•´ë°”ë¼ê¸°ì”¨", "í˜¸ë°•ì”¨", "ìºìŠˆë„›(ê²¬ê³¼ë¥˜ ì•ŒëŸ¬ì§€ ì—†ì„ ì‹œ)", "ë³‘ì•„ë¦¬ì½©"],
        "usage_tip": "ê³ ì†Œí•œ ë§›ì€ ë³¶ì€ ì”¨ì•—ë¥˜ë‚˜ ì½©ë¥˜ë¡œ ëŒ€ì²´ ê°€ëŠ¥í•©ë‹ˆë‹¤."
      },
      {
        "category": "ëŒ€ë‘",
        "items": ["ëŒ€ë‘"],
        "substitutes": ["ì™„ë‘ì½©", "ë³‘ì•„ë¦¬ì½©", "ì½”ì½”ë„› ì•„ë¯¸ë…¸ìŠ¤(ê°„ì¥ ëŒ€ì²´)", "í€´ë…¸ì•„"],
        "usage_tip": "ê°„ì¥ ëŒ€ì‹  ì½”ì½”ë„› ì•„ë¯¸ë…¸ìŠ¤ë¥¼ ì‚¬ìš©í•˜ë©´ ëŒ€ë‘ ì—†ì´ ê°ì¹ ë§›ì„ ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      },
      {
        "category": "ë°€",
        "items": ["ë°€"],
        "substitutes": ["ìŒ€ê°€ë£¨", "ê·€ë¦¬ê°€ë£¨", "íƒ€í”¼ì˜¤ì¹´ ê°€ë£¨", "ë©”ë°€ê°€ë£¨(ë©”ë°€ ì•ŒëŸ¬ì§€ ì—†ì„ ì‹œ)"],
        "usage_tip": "ê¸€ë£¨í… í”„ë¦¬ ê°€ë£¨ ë¯¹ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì ì„±ì„ ì¡°ì ˆí•˜ì„¸ìš”."
      },
      {
        "category": "ì£",
        "items": ["ì£"],
        "substitutes": ["í•´ë°”ë¼ê¸°ì”¨", "í˜¸ë°•ì”¨", "ë§ˆì¹´ë‹¤ë¯¸ì•„"],
        "usage_tip": "ë°”ì§ˆ í˜ìŠ¤í†  ë“± ì†ŒìŠ¤ ì œì‘ ì‹œ ì”¨ì•—ë¥˜ë¡œ ëŒ€ì²´ ê°€ëŠ¥í•©ë‹ˆë‹¤."
      },
      {
        "category": "í˜¸ë‘",
        "items": ["í˜¸ë‘"],
        "substitutes": ["í”¼ì¹¸", "í˜¸ë°•ì”¨", "ë³¶ì€ ê·€ë¦¬"],
        "usage_tip": "ì‹ê°ê³¼ í’ë¯¸ê°€ ìœ ì‚¬í•œ í”¼ì¹¸ì´ ê°€ì¥ ì¢‹ì€ ëŒ€ì•ˆì…ë‹ˆë‹¤."
      },
      {
        "category": "ê²Œ/ìƒˆìš°",
        "items": ["ê²Œ", "ìƒˆìš°"],
        "substitutes": ["í°ì‚´ ìƒì„ ", "ë²„ì„¯(í‚¹ì˜¤ì´ìŠ¤í„°)", "ë‘ë¶€"],
        "usage_tip": "íƒ±ê¸€í•œ ì‹ê°ì€ ë²„ì„¯ì´ë‚˜ ì–´ë¬µ(ì„±ë¶„ í™•ì¸ í•„ìˆ˜)ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤."
      },
      {
        "category": "ë¼ì§€ê³ ê¸°/ì‡ ê³ ê¸°/ë‹­ê³ ê¸°",
        "items": ["ë¼ì§€ê³ ê¸°", "ì‡ ê³ ê¸°", "ë‹­ê³ ê¸°"],
        "substitutes": ["ì½©ê³ ê¸°", "í…œí˜", "ë²„ì„¯ë¥˜", "ìƒì„ "],
        "usage_tip": "ë‹¨ë°±ì§ˆì›ì€ ì‹ë¬¼ì„± ë‹¨ë°±ì§ˆì´ë‚˜ ëŒ€ì²´ìœ¡ìœ¼ë¡œ ë³´ì¶©í•©ë‹ˆë‹¤."
      },
      {
        "category": "ë³µìˆ­ì•„/í† ë§ˆí† ",
        "items": ["ë³µìˆ­ì•„", "í† ë§ˆí† "],
        "substitutes": ["ì‚¬ê³¼", "ìë‘(ë³µìˆ­ì•„ ëŒ€ìš©)", "ë¹¨ê°„ íŒŒí”„ë¦¬ì¹´(í† ë§ˆí†  ëŒ€ìš©)"],
        "usage_tip": "í† ë§ˆí†  ì†ŒìŠ¤ì˜ ìƒ‰ê°ê³¼ ì‚°ë¯¸ëŠ” íŒŒí”„ë¦¬ì¹´ì™€ ì‹ì´ˆ ì¡°í•©ìœ¼ë¡œ í‰ë‚´ ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      },
      {
        "category": "ì•„í™©ì‚°ë¥˜",
        "items": ["ì•„í™©ì‚°ë¥˜(ì‚°í™”ë°©ì§€ì œ)"],
        "substitutes": ["ì²œì—° ë°œíš¨ ì‹ì´ˆ", "ë ˆëª¬ì¦™", "ìƒê³¼ì¼"],
        "usage_tip": "ê°€ê³µì‹í’ˆë³´ë‹¤ëŠ” ì‹ ì„  ì‹í’ˆ ìœ„ì£¼ì˜ ì„ íƒì´ í•„ìˆ˜ì ì…ë‹ˆë‹¤."
      },
      {
        "category": "ì¡°ê°œë¥˜/êµ´/ì „ë³µ/í™í•©",
        "items": ["ì¡°ê°œë¥˜", "êµ´", "ì „ë³µ", "í™í•©"],
        "substitutes": ["ë‹¤ì‹œë§ˆ", "í‘œê³ ë²„ì„¯", "ë©¸ì¹˜(ìƒì„  ì•ŒëŸ¬ì§€ ì—†ì„ ì‹œ)"],
        "usage_tip": "êµ­ë¬¼ ìš”ë¦¬ì˜ ê°ì¹ ë§›ì€ í•´ì¡°ë¥˜ì™€ ë²„ì„¯ìœ¼ë¡œ ì¶©ë¶„íˆ ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      },
      {
        "category": "ì˜¤ì§•ì–´",
        "items": ["ì˜¤ì§•ì–´"],
        "substitutes": ["ë¬¸ì–´(ì—°ì²´ë¥˜ ì•ŒëŸ¬ì§€ í™•ì¸)", "ë²„ì„¯", "ê³¤ì•½"],
        "usage_tip": "ì«„ê¹ƒí•œ ì‹ê°ì€ ë°ì¹œ ê³¤ì•½ì´ë‚˜ ë²„ì„¯ ê¸°ë‘¥ìœ¼ë¡œ ëŒ€ì²´ ê°€ëŠ¥í•©ë‹ˆë‹¤."
      }
    ]
  }

"""#ğŸ¤– ì—ì´ì „íŠ¸ ê°œìš”

START: ìœ ì €ê°€ [ì¥ë°”êµ¬ë‹ˆ ë‹´ê¸°] ë²„íŠ¼ì„ í´ë¦­í•¨

##1ï¸âƒ£ Node 1 (Orch-01): Eligibility Checking & Routing (Conditional Edge)

###1. ìœ ì €ê°€ ì§ˆë³‘ì„ ë³´ìœ í•˜ê³  ìˆëŠ”ê°€?
*@USER-Agent í˜¸ì¶œ
* IN: USER-01ì˜ Profile Retrieval
* Logic : PASS or WARN

      disease = diabetes + hypertension + kidneydisease + allergy

          ** PASS: disease = 0    -> end
          ** WARN: disease >= 1   -> 2. ì´ë™


###2. ì‹í’ˆì— ìœ ê´€ ì„±ë¶„ì´ ìˆëŠ”ê°€?
*@CHAT-Agent í˜¸ì¶œ
* IN: CHAT-01ì˜ Evidence Generation
* logic : PASS or WARN

          ** PASS: ë¶€ì í•©ì„±ë¶„ ì—†ìŒ    -> end,
          ** WARN: ë¶€ì í•©ì„±ë¶„ ìˆìŒ    -> to_chat, to_reco

* next: íŒì •ê²°ê³¼ì— ë”°ë¼ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ ë° tool í˜¸ì¶œ, ìƒí’ˆíŠ¹ì„± JSON ì „ë‹¬

#
#ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡
#

##2ï¸âƒ£ Node 2 (User-01): Profile Retrieval

ìœ ì €í”„ë¡œí•„ì„ ì¡°íšŒí•˜ì—¬ ê±´ê°•ì •ë³´ í™•ì¸ ë° í˜ë¥´ì†Œë‚˜ë¥¼ JSON ì¶œë ¥
* MVP: ?user_id?, weight, diabetes, hypertension, kidneydisease, allergy
* Post-MVP: height, activity_level, persona(FK, Post-MVP DB)



### ì°¸ê³ ) final_profile DB
Node2(User-02) + priority_map + ìœ ê´€ì„±ë¶„ ì„ê³„ê°’&ë¬¸ìì—´ í•„í„°ë§
* ë¯¸ë¦¬ ìƒì„±í•´ì„œ DBì— ì €ì¥ëœ ìƒíƒœ

Logic :
* ì„ê³„ê°’ ê¸°ë°˜ í•„í„°ë§

      diabetes: (-)ë‹¹(<5g)
                (+)ì‹ì´ì„¬ìœ (>=14g/1,000kcal)

      hypertension: (-)ë‚˜íŠ¸ë¥¨(<2,300mg/day),
                    (-)ì¹¼ë¥¨ > 3500mg
                    (-)ì´ ì§€ë°© < í•˜ë£¨ ì´ ì—´ëŸ‰ì˜ 20-25% ì´í•˜

      kidneydisease:  CKD 3-5ë‹¨ê³„ (íˆ¬ì„ ì „, ë‹¹ë‡¨ ì—†ìŒ)
                      (-)ë‹¨ë°±ì§ˆ kgë‹¹ 0.55-0.60g/kg/ì¼
                      (-)ë‚˜íŠ¸ë¥¨ 2.3g ë¯¸ë§Œ
                      (-)ì¸ 800-1,000 mg/ì¼
                      (-)ì¹¼ìŠ˜ 800-1,000mg/ì¼
                      (-)25-35 kcal/kg/ì¼

                      CKD 5ë‹¨ê³„ (íˆ¬ì„)
                      (+)ë‹¨ë°±ì§ˆ 1.0-1.2g/kg/ì¼
                      (-)ë‚˜íŠ¸ë¥¨ 2.3g ë¯¸ë§Œ
                      (-)ì¹¼ë¥¨ 2,000/ì¼ë¡œ ì œí•œ
                      (-)ì¸ 800-1,000 mg/ì¼
                      (-)ì¹¼ìŠ˜ 800-1,000mg/ì¼
                      (-)25-35 kcal/kg/ì¼
      
      (POST-MVPì—ì„œëŠ” (+) ì„±ë¶„ ì„­ì·¨ì‹œ ê¸ì •í”¼ë“œë°±ì„ ì£¼ëŠ”ê²ƒë„ ê³ ë ¤)

* ë¬¸ìì—´ ê¸°ë°˜ í•„í„°ë§

      allergy: ìš°ìœ , ë‹¬ê±€, ë•…ì½©, ê²¬ê³¼ë¥˜, ì½©, ë°€, ìƒì„ , ê°‘ê°ë¥˜ ì–´íŒ¨ë¥˜
      ì›ì¬ë£Œ í¬í•¨ì„±ë¶„ ë° êµì°¨ì˜¤ì—¼ì„±ë¶„

* ìš°ì„ ìˆœìœ„ ìˆœìœ¼ë¡œ

      priority_map = { allergy: 1, kidneydisease: 2, diabetes : 3, hypertension: 4 }

          ì§ˆë³‘ ìš°ì„ ìˆœìœ„ì— ë”°ë¥¸ í•„í„°ë§ì„±ë¶„ ë° ì„ê³„ê°’ì„ ë¨¼ì € í˜¸ì¶œ.
          ê·¸ ë‹¤ìŒ ìˆœìœ„ ì§ˆë³‘ì˜ í•„í„°ë§ì„±ë¶„ì„ í˜¸ì¶œ
          -> ì´ë¯¸ í˜¸ì¶œëœ í•„í„°ë§ì„±ë¶„ì€ ìƒˆë¡œ í˜¸ì¶œí•˜ì§€ ì•ŠìŒ
          Priority-Merger -> ì§ˆë³‘ ìš°ì„ ìˆœìœ„ì— ë”°ë¼ Final_Profile ìƒì„±

      final_profile = {
          user_id: ,
          ì„±ë¶„1: ì„±ë¶„1í•¨ëŸ‰,
          ì„±ë¶„2: ì„±ë¶„2í•¨ëŸ‰,
          ì•ŒëŸ¬ì§€: ì•ŒëŸ¬ì  
          ...

#
#ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡
#

##3ï¸âƒ£ Node 3 (Chat-01): Evidence Generation

ì‹í’ˆì„±ë¶„ê³¼ ì§ˆí™˜ ê°€ì´ë“œë¼ì¸ì„ ëŒ€ì¡°í•˜ì—¬ ê·¼ê±°ê¸°ë°˜ ì í•©ì„± íŒë³„í›„ *JSON ì¶œë ¥*
* IN: final_profile DB, product DB, disease RAG?

      *final_profile DB:
      *product DB: product_id, name, category, brand
                -> ì„±ë¶„í•¨ëŸ‰ì²´í¬ ë° ìƒí’ˆ ì¡°íšŒ ë§¥ë½ ìœ ì§€
      *disease RAG: ì§ˆë³‘ìœ ê´€ ì„±ë¶„ ë° ì„±ë¶„ ê°€ì´ë“œ(í•¨ëŸ‰)
                -> ì‹í’ˆ ì„±ë¶„ ì í•©ì„±
* logic : ì§ˆí™˜ë³„ ì„±ë¶„ í•„í„°ë§ (simple check)

* íŒë‹¨ê²°ê³¼ ì˜ˆì‹œ ----> **JSON ê¸°ë°˜ìœ¼ë¡œ ë³€ê²½ í•„ìš”

      Scenario : ğŸš« ì˜ì–‘ ì„±ë¶„ ì´ˆê³¼ (Nutrient Violation)
        - ìµœì¢… íŒì •: [WARN]
        - ìœ„ë°˜ ì•ŒëŸ¬ì§€: []
        - ìœ„ë°˜ ì˜ì–‘ì†Œ: ['sodium', 'sugar']
        - ìƒì„¸ ë¦¬í¬íŠ¸:
          âœ… PROTEIN: ì‹¤ì œ 15.0 / ê¸°ì¤€ 40.0
          ğŸš© SODIUM: ì‹¤ì œ 3000.0 / ê¸°ì¤€ 2300.0
          ğŸš© SUGAR: ì‹¤ì œ 10.0 / ê¸°ì¤€ 5.0





ìœ„ ì •ë³´ë¥¼ ë°›ì•„ì„œ recoë¡œ ë„˜ê¹€

##4ï¸âƒ£ Node 4 (Reco-01): Vector DB Search & Recommendation

* ìƒí’ˆíŠ¹ì„± JSONì„ ë°›ì•„ì„œ ìœ ì‚¬í•œ ìƒí’ˆ ì¡°íšŒ ë° ì¶”ì²œ

##4ï¸âƒ£ Node 4-1 (sub-Reco-01): Vector DB Search & Recommendation

ë°©ì‹1: ë ˆì½”ì—ì„œ ì¤€ 30-50ê°œ ì •ë„ì˜ ìƒí’ˆì¤‘ì—ì„œ roles.py ë¡œ 3ê°œë¥¼ ì¶”ì²œ
ë°©ì‹2: ë ˆì½”ì—ì„œëŠ” 3ê°œë§Œ ì£¼ê³  ê·¸ 3ê°œ ìƒí’ˆì˜ ì¡°íšŒë§¥ë½ê³¼ ì‹í’ˆì„±ë¶„ì´ ìœ ì €ì—ê²Œ ë§ëŠ”ì§€ íŒë‹¨

##5ï¸âƒ£ Node 5 (Resp-01):

#ğŸ¤– ì—ì´ì „íŠ¸

####ë¼ì´ë¸ŒëŸ¬ë¦¬
"""

from dataclasses import dataclass
from typing_extensions import TypedDict

from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph
from langgraph.runtime import Runtime

from langgraph.graph import START
from langgraph.graph import END

#orchagnet = RouterLogic(router_llm) - llm ì‚¬ìš©ì˜ í•©ë¦¬ì„± ê²€í†  í›„ ë¹„ì‚¬ìš©

useragent = ProfileRetrieval(retrieval_llm)

chatagent = EvidenceGeneration(generation_llm)

recoagent = Recommendation(recommendation_llm)

subsagent = ProductSubstitution(substitution_llm)

respagent = ResponseGeneration(response_llm)

"""#### ì—ì´ì „íŠ¸ë³„ ëª¨ë¸ ì„¤ì • ëª¨ìŒ"""

# main.py ë˜ëŠ” graph.py ìƒë‹¨
from langchain_openai import ChatOpenAI

# ë¼ìš°í„° ë¡œì§ì—ì„œëŠ” llm ì‚¬ìš© ì•ˆí•˜ê¸°ë¡œ
#router_llm = ChatOpenAI(
#    model="gpt-4o",
#    temperature=0,   # íŒë‹¨ì˜ ì¼ê´€ì„±ì„ ìœ„í•´ 0ìœ¼ë¡œ ì„¤ì •
#    api_key="sk-..." # (í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬)
#)



retrieval_llm = ChatOpenAI(model="gpt-4o", temperature=0)
generate_llm = ChatOpenAI(model="gpt-4o", temperature=0)
recommendation_llm = ChatOpenAI(model="gpt-4o", temperature=0)

"""#### ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ì—ì´ì „íŠ¸"""

class RouterLogic:
    def __init__(self):
        #
        pass

    def run(self, state: overallState) -> str:
        print("\nâš™ï¸ [Orch-Agent] ê·œì¹™ ê¸°ë°˜ ê²½ë¡œ íŒë‹¨ ì¤‘...")

        # 1. ë°ì´í„° ì¶”ì¶œ
        u_profile = state.get("user_profile", {})
        
        # chat_agentì—ì„œ ì„¤ì •ëœ ë¶„ì„ ê²°ê³¼ ì¶”ì¶œ
        any_exceed = state.get("any_exceed", False)
        any_allergen = state.get("any_allergen", False)

        # ì²´í¬í•  í”Œë˜ê·¸ ë¦¬ìŠ¤íŠ¸
        flag_keys = ["diabetes_flag", "hypertension_flag", "kidneydisease_flag", "allergy_flag"]

        # í”Œë˜ê·¸ ê°’ ì¶”ì¶œ (dict.getì„ ì‚¬ìš©í•˜ì—¬ í‚¤ê°€ ì—†ëŠ” ê²½ìš° None ì²˜ë¦¬)
        flags = [u_profile.get(key) for key in flag_keys]

        # --- [ì¡°ê±´ë¶„ê¸° ì‹œì‘] ---

        # ê·œì¹™ 1: í•„ìˆ˜ í”Œë˜ê·¸ ì¤‘ í•˜ë‚˜ë¼ë„ ëˆ„ë½(None)ëœ ê²½ìš°
        if any(f is None for f in flags):
            reason = "í•„ìˆ˜ ê±´ê°• ì •ë³´(Flags) ì¼ë¶€ ëˆ„ë½"
            return self._log_and_return("user_agent", reason)

        # ê·œì¹™ 2: WARN íŒì • - any_exceed ë˜ëŠ” any_allergenì´ trueì¸ ê²½ìš°
        # chat_agentê°€ ì‹¤í–‰ë˜ê³  í”¼ë“œë°± ë£¨í”„ë¡œ ëŒì•„ì™”ì„ ë•Œ ì´ ì¡°ê±´ì´ í™œì„±í™”ë¨
        if any_exceed or any_allergen:
            reason = f"ìœ„í—˜ ì„±ë¶„ ê°ì§€ (ì˜ì–‘ì„±ë¶„ ì´ˆê³¼: {any_exceed}, ì•ŒëŸ¬ì§€: {any_allergen})"
            return self._log_and_return("reco_agent", reason)

        # ê·œì¹™ 3: í”Œë˜ê·¸ê°€ ëª¨ë‘ ì¡´ì¬í•  ë•Œ í•©ê³„ ê³„ì‚°
        # 0ê³¼ 1ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤ê³  ê°€ì • (True/Falseì—¬ë„ sum ê°€ëŠ¥)
        flag_sum = sum(int(f) for f in flags)

        if flag_sum >= 1:
            return self._log_and_return("chat_agent", f"ì§ˆí™˜/ì•ŒëŸ¬ì§€ ë³´ìœ  ({flag_sum}ë‹¨ê³„)")
        else:
            return self._log_and_return("end", "ì§ˆí™˜/ì•ŒëŸ¬ì§€ ì—†ìŒ (ì •ìƒ)")

    def _log_and_return(self, next_step, reason):
        print(f"ğŸ‘‰ íŒë‹¨ ê²°ê³¼: {next_step} (ì´ìœ : {reason})")
        return next_step





"""#### ìœ ì € ì—ì´ì „íŠ¸"""

from typing import List, Dict, Any, TypedDict
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_core.runnables import RunnableConfig



# 1. LLM êµ¬ì¡°í™” ì¶œë ¥ì„ ìœ„í•œ Pydantic ëª¨ë¸
class HealthAnalysis(BaseModel):
    """ì§ˆë³‘ ì •ë³´ì— ê¸°ë°˜í•œ ì‹ì´ ê°€ì´ë“œ ë° ì£¼ì˜ ì„±ë¶„"""
    guidelines: List[str] = Field(
        ...,
        description="í™˜ìì˜ ì§ˆë³‘ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ êµ¬ì²´ì ì¸ ì‹ë‹¨ ê°€ì´ë“œë¼ì¸ 3~5ê°€ì§€"
    )
    target_ingredients: List[str] = Field(
        ...,
        description="í•´ë‹¹ ì§ˆë³‘ ë³´ìœ ìê°€ í”¼í•´ì•¼ í•˜ê±°ë‚˜ ì£¼ì˜í•´ì•¼ í•  í•µì‹¬ ì„±ë¶„ëª… ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ë‚˜íŠ¸ë¥¨, ë‹¹ë¥˜, í¬í™”ì§€ë°©)"
    )

# 2. User Agent í´ë˜ìŠ¤ (ProfileRetrieval)
class ProfileRetrieval:
    def __init__(self, model):
        """
        Args:
            model: êµ¬ì¡°í™”ëœ ì¶œë ¥ì„ ì§€ì›í•˜ëŠ” LLM ê°ì²´ (ì˜ˆ: ChatOpenAI)
        """
        self.llm = model
        # LLMì´ Pydantic ëª¨ë¸ì— ë§ì¶° ë‹µí•˜ë„ë¡ ì„¤ì •
        self.analysis_chain = self.llm.with_structured_output(HealthAnalysis)

    def _fetch_profile_from_backend(self, user_id: str) -> Dict[str, Any]:
        """
        [Mock API] user_idë¥¼ 'Key'ë¡œ ì‚¬ìš©í•˜ì—¬ ë°±ì—”ë“œ DBì—ì„œ ìœ ì € ì •ë³´ë¥¼ ì¡°íšŒí•¨.
        ì‹¤ì œë¡œëŠ” requests.get(f"api/user/{user_id}") í˜•íƒœê°€ ë¨.
        """
        print(f"ğŸ“¡ [User-Agent] ë°±ì—”ë“œ ì¡°íšŒ ì¤‘... (Target ID: {user_id})")

        # ì„ì‹œ DB (Mock Data)
        mock_db = {
            "user_001": {
                "name": "ê¹€ì² ìˆ˜",
                "diabetes": 1,        # ë‹¹ë‡¨ ìˆìŒ
                "hypertension": 1,    # ê³ í˜ˆì•• ìˆìŒ
                "kidneydisease": 0,
                "allergy": 0
            },
            "user_002": {
                "name": "ì´ì˜í¬",
                "diabetes": 0,
                "hypertension": 0,
                "kidneydisease": 1,   # ì‹ ì¥ì§ˆí™˜ ìˆìŒ
                "allergy": 1          # ì•ŒëŸ¬ì§€ ìˆìŒ
            }
        }

        # DBì— ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ë°˜í™˜
        return mock_db.get(user_id, {
            "name": "Unknown", "diabetes": 0, "hypertension": 0, "kidneydisease": 0, "allergy": 0
        })

    def run(self, state: OverallState, config: RunnableConfig = None) -> Dict[str, Any]:
        """LangGraph ë…¸ë“œ ì‹¤í–‰ í•¨ìˆ˜"""

        # 1. Stateì—ì„œ user_id êº¼ë‚´ê¸° (ì´ë¯¸ ìˆëŠ” ì •ë³´ í™œìš©)
        current_user_id = state.get("user_id")

        # 2. ë°±ì—”ë“œ APIë¥¼ í†µí•´ ìƒì„¸ í”„ë¡œí•„(ì§ˆë³‘ ìœ ë¬´) ê°€ì ¸ì˜¤ê¸°
        user_profile = self._fetch_profile_from_backend(current_user_id)

        # 3. LLMì—ê²Œ ë¶„ì„ ìš”ì²­ (0/1 ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ ê°€ì´ë“œë¡œ ë³€í™˜)
        #    í”„ë¡¬í”„íŠ¸ì— ì§ˆë³‘ ì •ë³´ë¥¼ ìš”ì•½í•´ì„œ ì „ë‹¬
        health_summary = (
            f"ë‹¹ë‡¨: {'ìˆìŒ' if user_profile['diabetes'] else 'ì—†ìŒ'}, "
            f"ê³ í˜ˆì••: {'ìˆìŒ' if user_profile['hypertension'] else 'ì—†ìŒ'}, "
            f"ì‹ ì¥ì§ˆí™˜: {'ìˆìŒ' if user_profile['kidneydisease'] else 'ì—†ìŒ'}, "
            f"ì•ŒëŸ¬ì§€: {'ìˆìŒ' if user_profile['allergy'] else 'ì—†ìŒ'}"
        )

        system_msg = "ë‹¹ì‹ ì€ ì„ìƒ ì˜ì–‘ì‚¬ì…ë‹ˆë‹¤. í™˜ìì˜ ì§ˆë³‘ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹ë‹¨ ê°€ì´ë“œì™€ ì£¼ì˜ ì„±ë¶„ì„ ì¶”ì¶œí•˜ì„¸ìš”."

        # LLM í˜¸ì¶œ (êµ¬ì¡°í™”ëœ ì¶œë ¥ ë°˜í™˜)
        analysis_result: HealthAnalysis = self.analysis_chain.invoke([
            SystemMessage(content=system_msg),
            HumanMessage(content=f"í™˜ì ì •ë³´: {health_summary}")
        ])

        print(f"âœ… [User-Agent] ë¶„ì„ ì™„ë£Œ: {analysis_result.target_ingredients}")

        # 4. State ì—…ë°ì´íŠ¸ (ìŠ¤í‚¤ë§ˆì— ë§ì¶°ì„œ ë°˜í™˜)
        return {
            # (1) ë°±ì—”ë“œì—ì„œ ê°€ì ¸ì˜¨ Raw Data
            "name": user_profile["name"],
            "diabetes": user_profile["diabetes_flag"],
            "hypertension": user_profile["hypertension_flag"],
            "kidneydisease": user_profile["kidneydisease_flag"],
            "allergy": user_profile["allergy_flag"],

            "diabetes_type": user_profile["diabetes_detail"],
            "hypertension_type": user_profile["hypertension_detail"],
            "kidneydisease_type": user_profile["kidneydisease_detail"],
            "allergy_list": user_profile["allergy_list"],

            "final_profile": user_profile, # ì„ê³„ê°’

            # (4) íë¦„ ì œì–´ (ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ì—ê²Œ í„´ì„ ë„˜ê¹€)
            "next_step": "orch_agent"
        }

# --- ì‚¬ìš© ì˜ˆì‹œ ---

# 1. LLM ì •ì˜ (ì•ì„œ ë§Œë“  router_llmê³¼ ê°™ì€ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥)
# from langchain_openai import ChatOpenAI
# retrieval_llm = ChatOpenAI(model="gpt-4o", temperature=0)

# 2. í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
useragent = ProfileRetrieval(retrieval_llm)

# 3. LangGraphì— ë…¸ë“œ ì¶”ê°€ ì‹œ
# workflow.add_node("user_agent", useragent.run)




"""#### ì±— ì• ë„ë¦¬ì‹œìŠ¤ ì—ì´ì „íŠ¸"""


import json

class EvidenceGeneration1:
    def __init__(self, model, tokenizer, final_profiles=None, products=None):
        self.llm = model # For LangChain compatibility if needed, though not used in generate_prompt current logic
        self.model = model
        self.tokenizer = tokenizer
        # ë°ì´í„°ë¥¼ í´ë˜ìŠ¤ ì†ì„±ìœ¼ë¡œ ì €ì¥
        self.final_profiles = final_profiles if final_profiles is not None else {}
        self.products = products if products is not None else {}

# 1. ë‹¹ë‡¨, ê³ í˜ˆì••, ì‹ ë¶€ì „ ë¶„ì„
    def evaluate_threshold1(self, final_profile_key, product_key) -> overallState:
        """
        ì˜ì–‘ì„±ë¶„ ì„ê³„ê°’ ì´ˆê³¼ ì—¬ë¶€ë¥¼ ë¶„ì„í•˜ì—¬ overallState í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        profile = self.final_profiles.get(str(final_profile_key), {})
        product = self.products.get(str(product_key), {})

        # ë””ë²„ê¹…: ë°ì´í„° í™•ì¸
        print(f"\n=== ë””ë²„ê¹… ì •ë³´ ===")
        print(f"final_profile_key: {final_profile_key} (type: {type(final_profile_key)})")
        print(f"product_key: {product_key} (type: {type(product_key)})")
        print(f"profile ë°ì´í„°: {profile}")
        print(f"product ë°ì´í„°: {product}")
        print(f"self.final_profilesì˜ í‚¤ë“¤: {list(self.final_profiles.keys())}")
        print(f"self.productsì˜ í‚¤ë“¤: {list(self.products.keys())}")

        # 1. overallState êµ¬ì¡°ì— ë§ê²Œ ê²°ê³¼ ê°ì²´ ì´ˆê¸°í™”
        state: overallState = {
            "any_exceed": False,
            "exceeded_nutrients": []
        }

        # 2. ë¶„ì„ ì œì™¸ í‚¤ ì„¤ì •
        exclude_keys = ['user_id', 'restricted_ingredients']
        target_nutrients = [k for k in profile.keys() if k not in exclude_keys]
        
        print(f"target_nutrients (ë¶„ì„ ëŒ€ìƒ): {target_nutrients}")

        # 3. ì˜ì–‘ì„±ë¶„ ì „ìˆ˜ ì¡°ì‚¬
        for nutrient in target_nutrients:
            limit = profile[nutrient]
            
            # ì»¬ëŸ¼ ì´ë¦„ì´ ë™ì¼í•˜ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš© (fat_ratioë§Œ ì˜ˆì™¸)
            actual = product.get(nutrient, 0)
            
            print(f"\n[ì²´í¬] {nutrient}: ê¸°ì¤€={limit}, ì‹¤ì œ={actual}")

            # fat_ratioëŠ” ë¹„ìœ¨ ê³„ì‚° í•„ìš” (ì§€ë°© ì¹¼ë¡œë¦¬ / ì´ ì¹¼ë¡œë¦¬)
            if nutrient == 'fat_ratio':
                total_calories = product.get('calories', 0)
                fat_calories = product.get('fat', 0) * 9  # ì§€ë°© 1g = 9kcal
                if total_calories > 0:
                    actual_ratio = fat_calories / total_calories
                    print(f"  ì§€ë°© ë¹„ìœ¨ ê³„ì‚°: {fat_calories}kcal / {total_calories}kcal = {actual_ratio:.3f}")
                    if actual_ratio > limit:
                        print(f"  âŒ ë¹„ìœ¨ ì´ˆê³¼: {actual_ratio:.3f} > {limit}")
                        state["any_exceed"] = True
                        state["exceeded_nutrients"].append(nutrient)
                    else:
                        print(f"  âœ… ë¹„ìœ¨ ì ì •")
            # ì¼ë°˜ì ì¸ ê²½ìš°: ê¸°ì¤€ ì´ˆê³¼ ì—¬ë¶€ í™•ì¸ (Exceed)
            else:
                if actual > limit:
                    print(f"  âŒ ê¸°ì¤€ ì´ˆê³¼: {actual} > {limit}")
                    state["any_exceed"] = True
                    state["exceeded_nutrients"].append(nutrient)
                else:
                    print(f"  âœ… ê¸°ì¤€ ì´í•˜")
        
        print(f"\n=== ìµœì¢… ê²°ê³¼ ===")
        print(f"any_exceed: {state['any_exceed']}")
        print(f"exceeded_nutrients: {state['exceeded_nutrients']}")
        print(f"==================\n")

        return state


# 2. ì•ŒëŸ¬ì§€ ë¶„ì„


    def generate_allergy_prompt1(self, product_key, final_profile_key, tone_key=None, max_new_tokens=512):
        """
        ì •ì˜ëœ ëª¨ë“ˆì˜ Key ê°’ì„ ë°›ì•„ ìµœì í™”ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        """
        # 2.1. í´ë˜ìŠ¤ ì™¸ë¶€ ë³€ìˆ˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ë°ì´í„°ê°€ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ get ì‚¬ìš©)
        p = products.get(str(product_key))
        f = final_profiles.get(str(final_profile_key))
        sub_rules = allergy_substitution_rules

        # 2.2. ì˜ˆì™¸ ì²˜ë¦¬: ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° Noneì´ ì•„ë‹Œ ëª…í™•í•œ ë¬¸ìì—´ ë°˜í™˜
        if not p or not f:
            return f"Error: ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (Product: {product_key}, Profile: {final_profile_key})"

        # 2.3. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        system_msg = f"""ë‹¹ì‹ ì€ ì‹í’ˆ ì„±ë¶„ ë° í™”í•™ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        ì£¼ì–´ì§„ ì›ì¬ë£Œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ìì˜ ì œí•œ ì‚¬í•­('restricted_ingredients')ê³¼ ëŒ€ì¡°í•˜ê³  ì†”ë£¨ì…˜ì„ ì œê³µí•˜ì„¸ìš”.

        ### [Layer 1] ì‹ì•½ì²˜ 22ì¢… ë§ˆìŠ¤í„° ë¦¬ìŠ¤íŠ¸ ê¸°ì¤€
        ë¶„ì„ ê¸°ì¤€ì€ ëŒ€í•œë¯¼êµ­ ì‹ì•½ì²˜ ê³ ì‹œ ì•Œë ˆë¥´ê¸° ìœ ë°œ ë¬¼ì§ˆ 22ì¢…ì…ë‹ˆë‹¤.

        ### [Layer 2] ì›ì¬ë£Œ ì¶”ë¡  ë° ë§¤í•‘ ê·œì¹™
        1. ì„±ë¶„ëª…ì— ì§ì ‘ì ì¸ ì´ë¦„ì´ ì—†ë”ë¼ë„ 'í•µì‹¬ ê¸°ì›(Source) ë¬¼ì§ˆ'ì„ ì‹ë³„í•©ë‹ˆë‹¤. (ì˜ˆ: 'ì¹´ì œì¸ë‚˜íŠ¸ë¥¨' -> 'ìš°ìœ ')
        2. ëŒ€ì²´ ì‹ì¬ë£ŒëŠ” ë‹¤ìŒ ê°€ì´ë“œë¥¼ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤:
        {sub_rules.get('rules', [])}

        ### [Layer 3] ë¶„ì„ ê°€ì´ë“œë¼ì¸
        - **Priority**: 1ìˆœìœ„ ì•ŒëŸ¬ì§€ ì°¨ë‹¨ / 2ìˆœìœ„ ì§ˆí™˜ ì˜ì–‘ ìˆ˜ì¹˜ ì¶©ì¡± ì—¬ë¶€.
        - **Severity Level**: Critical(í•¨ìœ ), Warning(êµì°¨ ì˜¤ì—¼ ê°€ëŠ¥ì„±), Safe(ë¬´ê´€) ë¶„ë¥˜.
        - ** ì•ŒëŸ¬ì§€ë¥¼ ìœ ë°œí•  ìˆ˜ ìˆëŠ” ëª¨ë“  ì›ì¬ë£Œë¥¼ í‘œì‹œí•˜ì„¸ìš”.
        - ** ë‹¤ë¥¸ì‚¬ëŒì—ê²Œ ì•ŒëŸ¬ì§€ ê°€ëŠ¥ì„± ìˆë‹¤ ë¼ëŠ” ì‹ì˜ í‘œí˜„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
        - ** ì¶”ë¡  ê·œì¹™ì„ ë“œëŸ¬ë‚´ì§€ ë§ˆì„¸ìš”.
        - ** ë°˜ë“œì‹œ ì•„ë˜ì˜ json í˜•íƒœë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì£¼ì–´ì§„ json ì»¬ëŸ¼ ì™¸ì—ëŠ” ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”.


        ### [ì¶œë ¥ í˜•ì‹]
        {{
          "ingredient_analysis": [
            {{
              "detected_ingredient": "ì„±ë¶„ëª…",
              "derived_from": "ê¸°ì›ë¬¼ì§ˆ(22ì¢… ê¸°ì¤€)",
              "substitute": "ì¶”ì²œ ëŒ€ì²´ì¬",
              "is_allergen": true
            }}
          ],
          "safety_summary": "ìµœì¢… ì„­ì·¨ ê°€ëŠ¥ ì—¬ë¶€ ë° ì£¼ì˜ì‚¬í•­",
        }}

        [ì˜ˆì‹œ] few-shot
        {{
          "ingredient_analysis": [
            {{
              "detected_ingredient": "ìƒˆìš°",
              "derived_from": "ìƒˆìš°",
              "substitute": "í°ì‚´ ìƒì„ , ë²„ì„¯(í‚¹ì˜¤ì´ìŠ¤í„°), ë‘ë¶€",
              "is_allergen": false
            }}
          ],
          "safety_summary": "ì´ ì œí’ˆì€ ì‚¬ìš©ìì˜ ì•ŒëŸ¬ì§€ í•­ëª©ì¸ ìš°ìœ ì™€ ë•…ì½©ì„ í¬í•¨í•˜ê³  ìˆì§€ ì•Šìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì„­ì·¨ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
        }}



        """

        # 2.4. ìœ ì € í”„ë¡¬í”„íŠ¸
        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ì˜ ë³€ë™ì„±ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ë™ì ë°ì´í„°ë¥¼ ìœ ì € ë©”ì‹œì§€ë¡œ ëª°ì•„ë„£ìŠµë‹ˆë‹¤.
        user_msg = f"""
        [ìƒí’ˆ ì •ë³´]
        - ìƒí’ˆëª…: {p.get('name', 'ì•Œ ìˆ˜ ì—†ìŒ')}
        - ì›ì¬ë£Œ: {p.get('ingerdients', [])}
        - ì œì¡°ì‚¬ ì£¼ì˜ì‚¬í•­: {p.get('allergy', 'ì—†ìŒ')} / {p.get('trace', 'ì—†ìŒ')}

        [ìœ ì € í”„ë¡œí•„]
        - ì œí•œ ì„±ë¶„: {f.get('restricted_ingredients')}
        """



        messages = [
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": user_msg}
        ]



        # 3. llm í† í¬ë‚˜ì´ì§•
        # chat template ì ìš© â†’ BatchEncoding ë°˜í™˜
        inputs = self.tokenizer.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,
            return_tensors="pt"
        ).to(self.model.device)

        attention_mask = inputs.get("attention_mask", None)
        if attention_mask is not None:
           attention_mask = attention_mask.to(self.model.device)

        with torch.no_grad():
             outputs = self.model.generate(
                input_ids=inputs["input_ids"],
                attention_mask=attention_mask,
                max_new_tokens=max_new_tokens,
                temperature=0.1,
                top_p=0.9,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id
        )

        # í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ì´í›„ë§Œ ë””ì½”ë”©
        # 3.1. ì§ˆë¬¸ì˜ ê¸¸ì´ë¥¼ ì½ë‹ˆë‹¤.
        prompt_length = inputs["input_ids"].shape[-1]

        # 3.2. ìŠ¬ë¼ì´ì‹±: ì „ì²´ ê²°ê³¼ì—ì„œ 10ë²ˆì§¸ ì´í›„ë¶€í„°ë§Œ ê°€ì ¸ì˜µë‹ˆë‹¤.
        generated_ids = outputs[0][prompt_length:]

        # 3.3. ë‹µë³€ë§Œ ë‚¨ì€ generated_idsë¥¼ ê¸€ìë¡œ ë°”ê¿‰ë‹ˆë‹¤.
        raw_response = self.tokenizer.decode(generated_ids, skip_special_tokens=True)
        
        # ë””ë²„ê¹…: ì›ë³¸ ì‘ë‹µ ì¶œë ¥
        print(f"\n=== ì›ë³¸ LLM ì‘ë‹µ ===")
        print(raw_response)
        print(f"===================\n")

        # overallState ì´ˆê¸°í™”
        state: overallState = {
            "any_allergen": False,
            "substitute": []
        }

        try:
            # JSON ì¶”ì¶œ: ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±° ë° JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
            response_text = raw_response.strip()
            
            # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±° (```json ... ``` í˜•íƒœ)
            if response_text.startswith("```"):
                # ì²« ë²ˆì§¸ ì¤„ ì œê±°
                lines = response_text.split('\n')
                if lines[0].startswith("```"):
                    lines = lines[1:]
                if lines and lines[-1].strip() == "```":
                    lines = lines[:-1]
                response_text = '\n'.join(lines)
            
            # ì²« ë²ˆì§¸ { ë¶€í„° ë§ˆì§€ë§‰ } ê¹Œì§€ ì¶”ì¶œ
            start_idx = response_text.find('{')
            end_idx = response_text.rfind('}')
            
            if start_idx == -1 or end_idx == -1:
                print("Error: JSON í˜•ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return state
            
            json_text = response_text[start_idx:end_idx+1]
            
            # íŠ¸ë ˆì¼ë§ ì½¤ë§ˆ ì œê±° (JSON íŒŒì‹± ì—ëŸ¬ì˜ í”í•œ ì›ì¸)
            import re
            json_text = re.sub(r',\s*}', '}', json_text)
            json_text = re.sub(r',\s*]', ']', json_text)
            
            print(f"\n=== ì¶”ì¶œëœ JSON ===")
            print(json_text)
            print(f"===================\n")
            
            # LLMì˜ ì¶œë ¥ì´ JSON í˜•ì‹ì´ë¯€ë¡œ íŒŒì‹± ì‹œë„
            data = json.loads(json_text)

            analysis = data.get("ingredient_analysis", [])

            # any_allergen ì¶”ì¶œ: ë¶„ì„ ê²°ê³¼ ì¤‘ í•˜ë‚˜ë¼ë„ trueê°€ ìˆìœ¼ë©´ true
            state["any_allergen"] = any(item.get("is_allergen", False) for item in analysis)

            # substitute ì¶”ì¶œ: is_allergenì´ trueì¸ í•­ëª©ì˜ ëŒ€ì²´ì¬ë§Œ ìˆ˜ì§‘ (ì¤‘ë³µ ì œê±°)
            sub_list = []
            for item in analysis:
                # is_allergenì´ trueì¸ í•­ëª©ë§Œ ì²˜ë¦¬
                if item.get("is_allergen", False):
                    sub = item.get("substitute", "")
                    if sub and sub != "ì—†ìŒ":
                        # "ìƒì„ , ë‘ë¶€" ì²˜ëŸ¼ ë¬¸ìì—´ë¡œ ì˜¬ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ë¶„ë¦¬
                        sub_list.extend([s.strip() for s in sub.split(',')])

            state["substitute"] = list(set(sub_list))  # ì¤‘ë³µ ì œê±°
            
            print(f"\n=== ìµœì¢… íŒŒì‹± ê²°ê³¼ ===")
            print(f"any_allergen: {state['any_allergen']}")
            print(f"substitute: {state['substitute']}")
            print(f"===================\n")

        except json.JSONDecodeError as e:
            print(f"Parsing Error: {e}")
            print(f"ë¬¸ì œê°€ ëœ í…ìŠ¤íŠ¸ (ì• 200ì): {json_text[:200] if 'json_text' in locals() else raw_response[:200]}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ state ë°˜í™˜

        return state








"""ì‹í’ˆì˜ ì˜ì–‘ì„±ë¶„ ì •ë³´ê°€ ì—†ìŒ

#### ë ˆì»¤ë©˜ë°ì´ì…˜ ì—ì´ì „íŠ¸

#### ì„­ìŠ¤í‹°íŠœì…˜ ì—ì´ì „íŠ¸ : Tool calling

ì¡°ê±´ì‹ìœ¼ë¡œ ë¶„ê¸°í•˜ëŠ”ê±°ë‘
ì•„ë¬´ê±°ë‚˜ ì¹˜ë©´ ë„¤ê°€ ë­ê°€ í•„ìš”í•œì§€ íŒë‹¨í•´ì„œ callingí•´ì„œ ì¨ ê°€ ë‹¤ë¦„
ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ì´ ì•Œì•„ì„œ íˆ´ í˜¸ì¶œí•´ì„œ ì“°ê²Œí•˜ë ¤ë©´ tool calling

tool calling ê¸°ëŠ¥ì„ ì§€ì›í•˜ëŠ” ëª¨ë¸ì´ ìˆìŒ
ì•„ë‹Œ ê²½ìš° ollama ë˜í•‘ ì‚¬ìš©ê°€ëŠ¥
gpt api ì“°ë©´ ì •í™•í•˜ê³  ì‰½ê³  ë¹ ë¥´ê²Œ ì˜ ë¨
local model (ex) qwen -

ì…ë ¥ëœ ì •ë³´ì— ëŒ€í•´ ê°€ì¹˜íŒë‹¨ì„ í•˜ê±°ë‚˜ ì˜ˆì¸¡ì„ í•´ì„œ ë™ì‘í•´ì•¼í•¨

tool calling ì»¨ì…‰:
llmì´ í•˜ê¸° ì–´ë ¤ìš´ ë™ì‘ë“¤ -ëª…í™•í•œ ìˆ˜ì‹ì„ í†µí•œ ê³„ì‚°ì´ë¼ë˜ê°€ ì™¸ë¶€ í˜¸ì¶œì´ë¼ë˜ê°€ ê·¸ê±°ë¥¼ ë” ì˜í•˜ëŠ” ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ëŠ”ê²ƒ
"""

from langchain_core.tools import tool

@tool
def multiply(a: int, b: int) -> int:
  """Mutifly a and b"""
  return a * b

  #ë‚˜ì¤‘ì— êµ¬í˜„

"""# ğŸ”„ LangGraph

### NODE ê¸°ëŠ¥ ì •ì˜/ì„ ì–¸
"""

## ì›Œí¬í”Œë¡œìš° ì •ì˜ ë° ë…¸ë“œ&ì—£ì§€ ë“±ë¡

from langgraph.graph import StateGraph

workflow = StateGraph(overallState)

# orchestrator ë¡œì§ (policy.py ì˜ RouterLogic ì¸ìŠ¤í„´ìŠ¤ê°€ orchagentë¼ê³  ê°€ì •)
# ì½”ë“œ ìƒë‹¨ì—ì„œ ì¸ìŠ¤í„´ìŠ¤ê°€ ì—†ë‹¤ë©´ ì•„ë˜ì™€ ê°™ì´ ë§¤í•‘í•©ë‹ˆë‹¤.
# workflow.add_node("orch_agent", orchagent.run) 
# í˜„ì¬ ë…¸íŠ¸ë¶ ì½”ë“œì—ì„œëŠ” orchagent ë³€ìˆ˜ í• ë‹¹ì´ ì£¼ì„ì²˜ë¦¬ë˜ì–´ ìˆìœ¼ë¯€ë¡œ, 
# ê¸°ì¡´ ì´ˆê¸°í™”ëœ RouterLogic() ì‚¬ìš©
orchagent = RouterLogic()

workflow.add_node("orch_agent", orchagent.run)

workflow.add_node("user_agent", useragent.run)
workflow.add_node("chat_agent", chatagent.evaluate_threshold)

# user ë‹˜ ìš”ì²­ì‚¬í•­: reco_agent ì˜ ë…ìì  state ìœ ì§€ë¥¼ ìœ„í•´ ì¤‘ê°„ ë¸Œë¦¿ì§€ ë˜í¼ ì‚¬ìš© 
# ì´ ì½”ë“œëŠ” recoagent ê°€ State ë³´ì¡´ì„ ì–´ë–»ê²Œ í• ì§€ì— ë”°ë¼ ë³€ê²½í•  ìˆ˜ ìˆìœ¼ë‚˜,
# ì§€ê¸ˆì€ recoagent.run (ë˜ëŠ” ì •ì˜ëœ ë©”ì¸ ì§„ì…ì )ì„ í˜¸ì¶œí•œë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
# (ë§Œì•½ reco_node í•¨ìˆ˜ê°€ ë³„ë„ë¡œ ì •ì˜ë˜ì–´ ìˆë‹¤ë©´ í•´ë‹¹ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”)
workflow.add_node("reco_agent", recoagent.run if hasattr(recoagent, 'run') else lambda state: state)

workflow.add_node("sub_reco_agent", subsagent.run if hasattr(subsagent, 'run') else lambda state: state)
workflow.add_node("resp_agent", respagent.run if hasattr(respagent, 'run') else lambda state: state)

workflow.add_edge(START, "orch_agent")
workflow.add_edge("orch_agent", "user_agent")
workflow.add_edge("user_agent", "orch_agent")

workflow.add_edge("orch_agent", "chat_agent")
workflow.add_edge("chat_agent", "orch_agent")

workflow.add_edge("chat_agent", "reco_agent")

workflow.add_edge("orch_agent", "reco_agent")
workflow.add_edge("reco_agent", "sub_reco_agent")
workflow.add_edge("sub_reco_agent", "reco_agent")
workflow.add_edge("sub_reco_agent", "resp_agent")




workflow.add_conditional_edges(
    "orch_agent",               # ì‹œì‘ ë…¸ë“œ
    lambda x: x.get("next_step", "end"),   # ë¼ìš°í„°ì˜ ê²°ê³¼ê°’(next_step)ì„ ì½ì–´ì„œ
    {                           # ì‹¤ì œ ì´ë™í•  ë…¸ë“œ ë§¤í•‘
        "user_agent": "user_agent",
        "chat_agent": "chat_agent",
        "reco_agent": "reco_agent",
        "sub_reco_agent": "sub_reco_agent",
        "resp_agent": "resp_agent",
        "end": END
    }
)




workflow.add_edge("orch_agent", END)
workflow.add_edge("resp_agent", END)


from typing import TypedDict, List








app = workflow.compile()

"""# ğŸ” ì‹œê°í™”"""

from IPython.display import Image, display

# workflow.compile()ì„ ë§ˆì¹œ app ê°ì²´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
try:
    display(Image(app.get_graph().draw_mermaid_png()))
except Exception:
    # í™˜ê²½ì— ë”°ë¼ ì¶”ê°€ ë¼ì´ë¸ŒëŸ¬ë¦¬(pyppeteer ë“±)ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    print("ê·¸ë˜í”„ë¥¼ ì‹œê°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

"""# ğŸ¶ ì•„í‚¤í…ì³ ì—°ê²°, ìµœì¢…ì¶œë ¥, ë™ì‘í™•ì¸"""

# ì´ˆê¸° ì…ë ¥ê°’ (ë¹ˆ ê°’ìœ¼ë¡œ ì‹œì‘í•´ë„ ë¨)
initial_input = {"user_id": "start_user"}

# ê·¸ë˜í”„ ì‹¤í–‰
final_state = app.invoke(initial_input)

# ìµœì¢… ê²°ê³¼ í™•ì¸
print("\n--- ìµœì¢… ìƒíƒœ ê²°ê³¼ ---")
print(final_state)

